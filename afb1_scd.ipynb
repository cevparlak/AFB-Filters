{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm","authorship_tag":"ABX9TyPHMra0zQ/YetcoEklAH1Qr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_0Sap5R72OP","executionInfo":{"status":"ok","timestamp":1709785228943,"user_tz":-180,"elapsed":26970,"user":{"displayName":"Salih Zeki Parlak","userId":"11379211521548434969"}},"outputId":"de3cb59c-c4ae-45e5-e76b-1d3063a50851"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["from __future__ import print_function\n","## Package\n","import IPython.display as ipd\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import plotly.graph_objs as go\n","import plotly.offline as py\n","import plotly.tools as tls\n","import seaborn as sns\n","import scipy.io.wavfile\n","import tensorflow as tf\n","py.init_notebook_mode(connected=True)\n","\n","import keras\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n","from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n","from keras.datasets import mnist\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Dropout, Activation, BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras import backend as K\n","from keras.utils import plot_model\n","from keras.regularizers import l2\n","from keras.optimizers import Adam\n","\n","import scipy.io as sio\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from scipy.io import arff\n","## Python\n","import random as rn\n","import sys\n","from sklearn import preprocessing\n","import glob\n","import os\n","# arr = os.listdir('D:\\\\emo\\\\emo2\\\\')\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","\n","#imports\n","from pandas import DataFrame\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","from matplotlib.collections import QuadMesh\n","import seaborn as sn\n","# import my_models\n","\n","## Package\n","import IPython.display as ipd\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import plotly.graph_objs as go\n","import plotly.offline as py\n","import plotly.tools as tls\n","import seaborn as sns\n","import scipy.io.wavfile\n","import tensorflow as tf\n","py.init_notebook_mode(connected=True)\n","\n","import keras\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n","from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n","from keras.datasets import mnist\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Dropout, Activation, BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras import backend as K\n","from keras.utils import plot_model\n","from keras.regularizers import l2\n","from keras.optimizers import Adam\n","\n","import scipy.io as sio\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from scipy.io import arff\n","## Python\n","import random as rn\n","import sys\n","from sklearn import preprocessing\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","#imports\n","from pandas import DataFrame\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","from matplotlib.collections import QuadMesh\n","import seaborn as sn\n","\n","def VGG16(input_shape, n_classes, acf):\n","## or use VGGNet\n","    return model, str1\n","\n","def get_new_fig(fn, figsize=[9,9]):\n","    \"\"\" Init graphics \"\"\"\n","    plt.clf()\n","    fig1 = plt.figure(fn, figsize)\n","    ax1 = fig1.gca()   #Get Current Axis\n","    ax1.cla() # clear existing plot\n","    return fig1, ax1\n","\n","def configcell_text_and_colors(array_df, lin, col, oText, facecolors, posi, fz, fmt, show_null_values=0):\n","    \"\"\"\n","      config cell text and colors\n","      and return text elements to add and to dell\n","      @TODO: use fmt\n","    \"\"\"\n","    text_add = []; text_del = [];\n","    cell_val = array_df[lin][col]\n","    tot_all = array_df[-1][-1]\n","    per = (float(cell_val) / tot_all) * 100\n","    curr_column = array_df[:,col]\n","    ccl = len(curr_column)\n","\n","    #last line  and/or last column\n","    if(col == (ccl - 1)) or (lin == (ccl - 1)):\n","        #tots and percents\n","        if(cell_val != 0):\n","            if(col == ccl - 1) and (lin == ccl - 1):\n","                tot_rig = 0\n","                for i in range(array_df.shape[0] - 1):\n","                    tot_rig += array_df[i][i]\n","                per_ok = (float(tot_rig) / cell_val) * 100\n","            elif(col == ccl - 1):\n","                tot_rig = array_df[lin][lin]\n","                per_ok = (float(tot_rig) / cell_val) * 100\n","            elif(lin == ccl - 1):\n","                tot_rig = array_df[col][col]\n","                per_ok = (float(tot_rig) / cell_val) * 100\n","            per_err = 100 - per_ok\n","        else:\n","            per_ok = per_err = 0\n","\n","        per_ok_s = ['%.2f%%'%(per_ok), '100%'] [per_ok == 100]\n","\n","        #text to DEL\n","        text_del.append(oText)\n","\n","        #text to ADD\n","        font_prop = fm.FontProperties(weight='bold', size=fz)\n","        text_kwargs = dict(color='w', ha=\"center\", va=\"center\", gid='sum', fontproperties=font_prop)\n","        lis_txt = ['%d'%(cell_val), per_ok_s, '%.2f%%'%(per_err)]\n","        lis_kwa = [text_kwargs]\n","        dic = text_kwargs.copy(); dic['color'] = 'g'; lis_kwa.append(dic);\n","        dic = text_kwargs.copy(); dic['color'] = 'r'; lis_kwa.append(dic);\n","        lis_pos = [(oText._x, oText._y-0.3), (oText._x, oText._y), (oText._x, oText._y+0.3)]\n","        for i in range(len(lis_txt)):\n","            newText = dict(x=lis_pos[i][0], y=lis_pos[i][1], text=lis_txt[i], kw=lis_kwa[i])\n","            #print 'lin: %s, col: %s, newText: %s' %(lin, col, newText)\n","            text_add.append(newText)\n","        #print '\\n'\n","\n","        #set background color for sum cells (last line and last column)\n","        carr = [0.27, 0.30, 0.27, 1.0]\n","        if(col == ccl - 1) and (lin == ccl - 1):\n","            carr = [0.17, 0.20, 0.17, 1.0]\n","        facecolors[posi] = carr\n","\n","    else:\n","        if(per > 0):\n","            txt = '%s\\n%.2f%%' %(cell_val, per)\n","        else:\n","            if(show_null_values == 0):\n","                txt = ''\n","            elif(show_null_values == 1):\n","                txt = '0'\n","            else:\n","                txt = '0\\n0.0%'\n","        oText.set_text(txt)\n","\n","        #main diagonal\n","        if(col == lin):\n","            #set color of the textin the diagonal to white\n","            oText.set_color('w')\n","            # set background color in the diagonal to blue\n","            facecolors[posi] = [0.35, 0.8, 0.55, 1.0]\n","        else:\n","            oText.set_color('r')\n","\n","    return text_add, text_del\n","\n","def insert_totals(df_cm):\n","    \"\"\" insert total column and line (the last ones) \"\"\"\n","    sum_col = []\n","    for c in df_cm.columns:\n","        sum_col.append( df_cm[c].sum() )\n","    sum_lin = []\n","    for item_line in df_cm.iterrows():\n","        sum_lin.append( item_line[1].sum() )\n","    df_cm['sum_lin'] = sum_lin\n","    sum_col.append(np.sum(sum_lin))\n","    df_cm.loc['sum_col'] = sum_col\n","    #print ('\\ndf_cm:\\n', df_cm, '\\n\\b\\n')\n","\n","def pretty_plot_confusion_matrix(df_cm, annot=True, cmap=\"Oranges\", fmt='.2f', fz=11,\n","      lw=0.5, cbar=False, figsize=[8,8], show_null_values=0, pred_val_axis='y'):\n","    \"\"\"\n","      print conf matrix with default layout (like matlab)\n","      params:\n","        df_cm          dataframe (pandas) without totals\n","        annot          print text in each cell\n","        cmap           Oranges,Oranges_r,YlGnBu,Blues,RdBu, ... see:\n","        fz             fontsize\n","        lw             linewidth\n","        pred_val_axis  where to show the prediction values (x or y axis)\n","                        'col' or 'x': show predicted values in columns (x axis) instead lines\n","                        'lin' or 'y': show predicted values in lines   (y axis)\n","    \"\"\"\n","    if(pred_val_axis in ('col', 'x')):\n","        xlbl = 'Predicted'\n","        ylbl = 'Actual'\n","    else:\n","        xlbl = 'Actual'\n","        ylbl = 'Predicted'\n","        df_cm = df_cm.T\n","\n","    # create \"Total\" column\n","    insert_totals(df_cm)\n","\n","    #this is for print allways in the same window\n","    fig, ax1 = get_new_fig('Conf matrix default', figsize)\n","\n","    #thanks for seaborn\n","    ax = sn.heatmap(df_cm, annot=annot, annot_kws={\"size\": fz}, linewidths=lw, ax=ax1,\n","                    cbar=cbar, cmap=cmap, linecolor='w', fmt=fmt)\n","\n","    #set ticklabels rotation\n","    ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, fontsize = 10)\n","    ax.set_yticklabels(ax.get_yticklabels(), rotation = 25, fontsize = 10)\n","\n","    # Turn off all the ticks\n","    for t in ax.xaxis.get_major_ticks():\n","        t.tick1On = False\n","        t.tick2On = False\n","    for t in ax.yaxis.get_major_ticks():\n","        t.tick1On = False\n","        t.tick2On = False\n","\n","    #face colors list\n","    quadmesh = ax.findobj(QuadMesh)[0]\n","    facecolors = quadmesh.get_facecolors()\n","\n","    #iter in text elements\n","    array_df = np.array( df_cm.to_records(index=False).tolist() )\n","    text_add = []; text_del = [];\n","    posi = -1 #from left to right, bottom to top.\n","    for t in ax.collections[0].axes.texts: #ax.texts:\n","        pos = np.array( t.get_position()) - [0.5,0.5]\n","        lin = int(pos[1]); col = int(pos[0]);\n","        posi += 1\n","        #print ('>>> pos: %s, posi: %s, val: %s, txt: %s' %(pos, posi, array_df[lin][col], t.get_text()))\n","\n","        #set text\n","        txt_res = configcell_text_and_colors(array_df, lin, col, t, facecolors, posi, fz, fmt, show_null_values)\n","\n","        text_add.extend(txt_res[0])\n","        text_del.extend(txt_res[1])\n","\n","    #remove the old ones\n","    for item in text_del:\n","        item.remove()\n","    #append the new ones\n","    for item in text_add:\n","        ax.text(item['x'], item['y'], item['text'], **item['kw'])\n","\n","    #titles and legends\n","    ax.set_title('Confusion matrix')\n","    ax.set_xlabel(xlbl)\n","    ax.set_ylabel(ylbl)\n","    plt.tight_layout()  #set layout slim\n","#    plt.show()\n","    plt.savefig(head1+'\\\\conf_mat_'+tail1+'_cnn.png')\n","\n","\n","def plot_confusion_matrix_from_data(y_test, predictions, columns=None, annot=True, cmap=\"Oranges\",\n","      fmt='.2f', fz=11, lw=0.5, cbar=False, figsize=[8,8], show_null_values=0, pred_val_axis='lin'):\n","    \"\"\"\n","        plot confusion matrix function with y_test (actual values) and predictions (predic),\n","        whitout a confusion matrix yet\n","    \"\"\"\n","    from sklearn.metrics import confusion_matrix\n","    from pandas import DataFrame\n","\n","    #data\n","    if(not columns):\n","        #labels axis integer:\n","        ##columns = range(1, len(np.unique(y_test))+1)\n","        #labels axis string:\n","        from string import ascii_uppercase\n","        columns = ['class %s' %(i) for i in list(ascii_uppercase)[0:len(np.unique(y_test))]]\n","        columns=['A','H','N','S']\n","        columns=labels\n","\n","    confm = confusion_matrix(y_test, predictions)\n","    cmap = 'Oranges';\n","    fz = 11;\n","    figsize=[9,9];\n","    show_null_values = 2\n","    df_cm = DataFrame(confm, index=columns, columns=columns)\n","    pretty_plot_confusion_matrix(df_cm, fz=fz, cmap=cmap, figsize=figsize, show_null_values=show_null_values, pred_val_axis=pred_val_axis)\n","#\n","#TEST functions\n","def _test_cm():\n","    #test function with confusion matrix done\n","    array = np.array( [[13,  0,  1,  0,  2,  0],\n","                       [ 0, 50,  2,  0, 10,  0],\n","                       [ 0, 13, 16,  0,  0,  3],\n","                       [ 0,  0,  0, 13,  1,  0],\n","                       [ 0, 40,  0,  1, 15,  0],\n","                       [ 0,  0,  0,  0,  0, 20]])\n","    #get pandas dataframe\n","    df_cm = DataFrame(array, index=range(1,7), columns=range(1,7))\n","    #colormap: see this and choose your more dear\n","    cmap = 'PuRd'\n","    pretty_plot_confusion_matrix(df_cm, cmap=cmap)\n","#\n","def _test_data_class(y_test1, y_pred1):\n","    \"\"\" test function with y_test (actual values) and predictions (predic) \"\"\"\n","    #data\n","#    y_test = np.array([1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5])\n","#    predic = np.array([1,2,4,3,5, 1,2,4,3,5, 1,2,3,4,4, 1,4,3,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,3,3,5, 1,2,3,3,5, 1,2,3,4,4, 1,2,3,4,1, 1,2,3,4,1, 1,2,3,4,1, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5])\n","    y_test = y_test1\n","    predic=y_pred1\n","    \"\"\"\n","      Examples to validate output (confusion matrix plot)\n","        actual: 5 and prediction 1   >>  3\n","        actual: 2 and prediction 4   >>  1\n","        actual: 3 and prediction 4   >>  10\n","    \"\"\"\n","    columns = []\n","    annot = True;\n","    cmap = 'Oranges';\n","    fmt = '.2f'\n","    lw = 0.5\n","    cbar = False\n","    show_null_values = 2\n","    pred_val_axis = 'y'\n","    #size::\n","    fz = 12;\n","    figsize = [9,9];\n","    if(len(y_test) > 10):\n","        fz=9; figsize=[14,14];\n","    plot_confusion_matrix_from_data(y_test, predic, columns,\n","      annot, cmap, fmt, fz, lw, cbar, figsize, show_null_values, pred_val_axis)\n","\n","# Set up Keras util functions\n","from keras import backend as K\n","\n","def precision(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def recall(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def fscore(y_true, y_pred):\n","    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n","        return 0\n","\n","    p = precision(y_true, y_pred)\n","    r = recall(y_true, y_pred)\n","    f_score = 2 * (p * r) / (p + r + K.epsilon())\n","    return f_score\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","model_list1=['mini_nvidia_model','nvidia_model','sVGG','VGG16','VGG19','ResNet v2','ResNet v1']\n","alist = pd.DataFrame(columns=['Model','Train', 'Test', 'Acc', 'Loss','Precision','Recall','F1','Kappa','ROC'])\n","nrows=100\n","batch_size1 = 32\n","epoch1 = 100\n","test_size1=0.30\n","lr1=0.001\n","factor1=0.90\n","patience1=20\n","acf='relu'\n","modelstr1='cnn_'+str(lr1)+'_'+str(factor1)+'_'+str(patience1)+'_'+str(nrows)+'_'+str(epoch1)+'_'+str(batch_size1)+'_'+acf\n","i=1\n","# np.random.seed(1234)\n","# session_conf = tf.compat.v1.ConfigProto( intra_op_parallelism_threads=1, inter_op_parallelism_threads=1 )\n","# sess = tf.compat.v1.Session( graph=tf.compat.v1.get_default_graph(), config=session_conf )\n","# tf.compat.v1.keras.backend.set_session(sess)\n","#-----------------------------Keras reproducibility------------------#\n","# SEED = 1234\n","\n","# tf.set_random_seed(SEED)\n","# os.environ['PYTHONHASHSEED'] = str(SEED)\n","# np.random.seed(SEED)\n","# rn.seed(SEED)\n","\n","# session_conf = tf.ConfigProto(\n","#     intra_op_parallelism_threads=1,\n","#     inter_op_parallelism_threads=1\n","# )\n","# sess = tf.Session(\n","#     graph=tf.get_default_graph(),\n","#     config=session_conf\n","# )\n","# K.set_session(sess)\n","#-----------------------------------------------------------------#\n","\n","path1=\"/content/gdrive/My Drive/timit/\"\n","path1=\"\"\n","#\n","\n","list1=[\n","'iemo full mfcc_emp3 _d __cons_512_256_AHNS.arff',\n","# 'spc all_efb_b6_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b7_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b8_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b9_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b10_x1__emp3_d_400_160_100_norm_type_1.npy',\n","'spc all_efb_b11_x1c6_v3__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b12_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b13_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b14_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b15_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b16_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b17_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b18_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b19_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b20_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b21_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b22_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b23_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b24_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_efb_b25_x1__emp3_d_400_160_100_norm_type_1.npy',\n","# 'spc all_mfcc___emp3_d_400_160_100_norm_type_1.npy'\n","# 'spc all_mel___emp3_d_400_160_100_norm_type_1.npy'\n","]\n","lr_list1=(0.001,0.001)\n","for cm in range(1):\n","    lr1=lr_list1[cm]\n","    print(lr1)\n","    for st in list1:\n","        # df=arff.loadarff(st)\n","\n","        x=np.load(path1+st)\n","        print(path1+st)\n","\n","        y=x[:,-1]\n","#    break\n","        x=x[:,:-1]\n","#    break\n","        x[np.isnan(x)] = 0\n","\n","        #    x[~np.all(x == 0, axis=1)]\n","    #    xx=np.any(np.isnan(x))\n","    # normalize each column independently between [0,1]\n","        min_max_scaler = preprocessing.MinMaxScaler()\n","        x= min_max_scaler.fit_transform(x)\n","#        del data\n","        # feature count must be greater than 40 or so for the cnn\n","        a1=x.shape[1]\n","        a2=a1 % nrows\n","        # print(a1)\n","        # print(a2)\n","        if a2!=0:\n","            a2=nrows-a2\n","            c1=np.zeros((x.shape[0],a2),dtype=int)\n","            x=np.concatenate((x,c1),axis=1)\n","        values, counts = np.unique(y, return_counts=True)\n","        n_classes=len(counts)\n","        ncols=int(x.shape[1]/nrows)\n","        x_train, x_test, y_train, y_test  = sklearn.model_selection.train_test_split(x, y, test_size=test_size1, random_state=1)\n","    #    xxx=x_train\n","        del x\n","        del y\n","        # print(nrows)\n","        # print(ncols)\n","\n","\n","        # input image dimensions\n","        img_rows, img_cols = nrows, ncols\n","        if K.image_data_format() == 'channels_first':\n","            x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","            x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","            input_shape = (1, img_rows, img_cols)\n","        else:\n","            x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","            input_shape = (img_rows, img_cols, 1)\n","\n","        # convert class vectors to binary class matrices\n","        y_train = y_train-1\n","        y_test = y_test-1\n","        y_test=y_test.astype(int)\n","        y_train = keras.utils.to_categorical(y_train, n_classes)\n","        y_test = keras.utils.to_categorical(y_test, n_classes)\n","\n","        print('x_train shape:', x_train.shape)\n","        print(x_train.shape[0], 'train samples')\n","        print(x_test.shape[0], 'test samples')\n","\n","        cm=3\n","\n","        # str1='mini_nvidia_model'\n","        # model = Sequential()\n","        # model.add(Conv2D(32, kernel_size=(3, 3), activation=acf, input_shape=input_shape))\n","        # model.add(Conv2D(64, (3, 3), activation=acf))\n","        # model.add(MaxPooling2D(pool_size=(2, 2)))\n","        # model.add(Dropout(0.25))\n","        # model.add(Flatten())\n","        # model.add(Dense(128, activation=acf))\n","        # model.add(Dropout(0.5))\n","        # model.add(Dense(n_classes, activation='softmax'))\n","\n","    #     str1='sVGG'\n","    #     chanDim = -1\n","    #     model = Sequential()\n","    #     # first CONV => RELU => CONV => RELU => POOL layer set\n","    #     model.add(Conv2D(32, (3, 3), padding = \"same\", input_shape=input_shape))\n","    #     model.add(Activation(\"relu\"))\n","    #     model.add(BatchNormalization(axis=chanDim))\n","    #     model.add(Conv2D(32, (3, 3), padding=\"same\"))\n","    #     model.add(Activation(\"relu\"))\n","    #     model.add(BatchNormalization(axis=chanDim))\n","    #     model.add(MaxPooling2D(pool_size=(2, 2)))\n","    #     model.add(Dropout(0.25))\n","\n","    #     # second CONV => Relu => CONV => Relu => POOL layer set\n","    #     model.add(Conv2D(64, (3, 3), padding=\"same\"))\n","    #     model.add(Activation(\"relu\"))\n","    #     model.add(BatchNormalization(axis=chanDim))\n","    #     model.add(Conv2D(64, (3, 3), padding=\"same\"))\n","    #     model.add(Activation(\"relu\"))\n","    #     model.add(BatchNormalization(axis=chanDim))\n","    #     model.add(MaxPooling2D(pool_size=(2, 2)))\n","    #     model.add(Dropout(0.25))\n","\n","    #     # third CONV => Relu => CONV => Relu => POOL layer set\n","    #     model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","    #     model.add(Activation(\"relu\"))\n","    #     model.add(BatchNormalization(axis=chanDim))\n","    #     model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","    #     model.add(Activation(\"relu\"))\n","    #     model.add(BatchNormalization(axis=chanDim))\n","    #     model.add(MaxPooling2D(pool_size=(2, 2)))\n","    #     model.add(Dropout(0.25))\n","\n","    #     # first (and only) set of FC => Relu layers\n","    #     model.add(Flatten())\n","    #     model.add(Dense(512))\n","    #     model.add(Activation(\"relu\"))\n","    #     model.add(BatchNormalization(axis=chanDim))\n","    #     model.add(Dropout(0.5))\n","    # #     softmax classifier\n","    #     model.add(Dense(n_classes, activation='softmax'))\n","\n","\n","# or use VGGNet\n","        str1='VGG16'\n","        chanDim = -1\n","        model = Sequential()\n","        # first CONV => RELU => CONV => RELU => POOL layer set\n","        model.add(Conv2D(64, (3, 3), padding = \"same\", input_shape=input_shape))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        # second CONV => Relu => CONV => Relu => POOL layer set\n","        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        # third CONV => Relu => CONV => Relu => POOL layer set\n","        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        # model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        # model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","        # first (and only) set of FC => Relu layers\n","        model.add(Flatten())\n","        model.add(Dense(512))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","\n","        model.add(Dense(256))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Dropout(0.5))\n","\n","        # softmax classifier\n","        model.add(Dense(n_classes, activation='softmax'))\n","\n","# ##  use VGGNet\n","#         str1='VGG19'\n","#         cm=4\n","#         chanDim = -1\n","#         model = Sequential()\n","#         # first CONV => RELU => CONV => RELU => POOL layer set\n","#         model.add(Conv2D(64, (3, 3), padding = \"same\", input_shape=input_shape))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(64, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(MaxPooling2D(pool_size=(2, 2)))\n","#         model.add(Dropout(0.25))\n","\n","#         # second CONV => Relu => CONV => Relu => POOL layer set\n","#         model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(MaxPooling2D(pool_size=(2, 2)))\n","#         model.add(Dropout(0.25))\n","\n","#         # third CONV => Relu => CONV => Relu => POOL layer set\n","#         model.add(Conv2D(256, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(256, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(256, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(256, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(MaxPooling2D(pool_size=(2, 2)))\n","#         model.add(Dropout(0.25))\n","\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Conv2D(512, (3, 3), padding=\"same\"))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","#         # first (and only) set of FC => Relu layers\n","#         model.add(Flatten())\n","#         model.add(Dense(512))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization())\n","#         model.add(Dropout(0.5))\n","\n","#         model.add(Dense(256))\n","#         model.add(Activation(\"relu\"))\n","#         model.add(BatchNormalization(axis=chanDim))\n","#         model.add(Dropout(0.5))\n","\n","#         # softmax classifier\n","#         model.add(Dense(n_classes, activation='softmax'))\n","\n","    #    model = my_models.VGG16(input_shape, n_classes)\n","        #model.compile(loss='categorical_crossentropy', optimizer=optimizer1, metrics=['accuracy'])\n","            #  optimizer = Adam(lr=1e-4)\n","        #    optimizer=keras.optimizers.RMSprop(lr=0.0001) #, rho=0.9, epsilon=None, decay=0.0)\n","        #    optimizer=keras.optimizers.Adadelta() #, epsilon=None, decay=0.0)\n","        #  optimizer=keras.optimizers.Adagrad(lr=0.001) #, epsilon=None, decay=0.0)\n","        #  optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","        #  optimizer=keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n","        #  optimizer=keras.optimizers.Nadam(lr=0.0001) #, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n","        # Model Training\n","        #    lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=factor1, patience=patience1, min_lr=0.000000001, verbose=1)\n","        # Please change the model name accordingly.\n","        #    mcp_save = ModelCheckpoint('model/aug_noiseNshift_2class2_np.h5', save_best_only=True, monitor='val_loss', mode='min')\n","        #optimizer = Adam(lr=1e-4)\n","\n","        # optimizer1 = keras.optimizers.SGD(learning_rate=lr1, momentum=0.0, nesterov=False)\n","        # lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=factor1, patience=patience1, min_lr=0.000000001, verbose=1)\n","        # #callbacks = [checkpoint, lr_reduce, lr_scheduler]\n","        # callbacks = [lr_reduce]\n","        # opt_name='sgd';\n","\n","        optimizer1=keras.optimizers.Adam(learning_rate=lr1, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","        callbacks = []\n","        opt_name='adamW';\n","\n","        model.compile(loss='categorical_crossentropy', optimizer=optimizer1, metrics=['accuracy'])\n","\n","        # Prepare model saving directory.\n","        # save_dir = os.path.join(os.getcwd(), 'saved_models')\n","        # model_name = 'best_model'+ modelstr1\n","        # if not os.path.isdir(save_dir):\n","        #     os.makedirs(save_dir)\n","        # filepath = os.path.join(save_dir, model_name)\n","        # Prepare callbacks for model saving and for learning rate adjustment.\n","        # checkpoint = ModelCheckpoint(filepath=filepath,\n","        #                              monitor='val_acc',\n","        #                              verbose=1,\n","        #                              save_best_only=True)\n","\n","        #lr_scheduler = LearningRateScheduler(lr_schedule)\n","        #lr_reduce2 = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n","        # model.summary()\n","\n","        # epoch1=10\n","        history=model.fit(x_train, y_train,\n","                      batch_size=batch_size1,\n","                      epochs=epoch1,\n","                      validation_data=(x_test, y_test),\n","                      shuffle=True,\n","                      callbacks=callbacks)\n","\n","        # model.save(path1+'model1')\n","        # model = keras.models.load_model(path1+'model1')\n","\n","\n","        # epoch1=40\n","        # history=model.fit(x_train, y_train,\n","        #               batch_size=batch_size1,\n","        #               epochs=epoch1,\n","        #               validation_data=(x_test, y_test),\n","        #               shuffle=True,\n","        #               callbacks=callbacks)\n","        # model.save(path1+'model1')\n","        # model = keras.models.load_model(path1+'model1')\n","\n","        # epoch1=50\n","        # history=model.fit(x_train, y_train,\n","        #               batch_size=batch_size1,\n","        #               epochs=epoch1,\n","        #               validation_data=(x_test, y_test),\n","        #               shuffle=True,\n","        #               callbacks=callbacks)\n","        # model.save(path1+'model1')\n","        # model = keras.models.load_model(path1+'model1')\n","\n","        # epoch1=100\n","        # history=model.fit(x_train, y_train,\n","        #               batch_size=batch_size1,\n","        #               epochs=epoch1,\n","        #               validation_data=(x_test, y_test),\n","        #               shuffle=True,\n","        #               callbacks=callbacks)\n","        # model.save(path1+'model1')\n","        # model = keras.models.load_model(path1+'model1')\n","\n","        # epoch1=100\n","        # history=model.fit(x_train, y_train,\n","        #               batch_size=batch_size1,\n","        #               epochs=epoch1,\n","        #               validation_data=(x_test, y_test),\n","        #               shuffle=True,\n","        #               callbacks=callbacks)\n","        # model.save(path1+'model1')\n","        # model = keras.models.load_model(path1+'model1')\n","\n","        score = model.evaluate(x_test, y_test, verbose=0)\n","        #    history=model.fit(x_train, y_train, batch_size = batch_size1, epochs=epoch1,  validation_data=(x_test, y_test), callbacks=[lr_reduce])\n","        #    score = model.evaluate(x_test, y_test, verbose=0)\n","        modelstr1='cnn_'+str(lr1)+'_'+str(factor1)+'_'+str(patience1)+'_'+str(nrows)+'_'+str(epoch1)+'_'+str(batch_size1)+'_'+acf\n","        modelstr2=model_list1[cm]+'_'+modelstr1+'_'+opt_name\n","\n","        print('Test loss:', score[0])\n","        print('Test accuracy:', score[1])\n","        head1, tail1 = os.path.split(path1+st)\n","\n","    # Plotting the Train Valid Loss Graph\n","        r1=8\n","        c1=6\n","        f1=16\n","        linew1=3\n","        plt.clf()\n","        plt.figure(figsize=(r1,c1))\n","        plt.rcParams.update({'font.size': f1})\n","        plt.plot(history.history['loss'],linewidth=linew1)\n","        plt.plot(history.history['val_loss'],linewidth=linew1)\n","        plt.title('Model Loss')\n","        plt.xlabel('epoch')\n","        plt.ylabel('loss')\n","        plt.legend(['train', 'test'])\n","    #    plt.legend(['training', 'test'], loc='upper left')\n","    #    plt.show()\n","        plt.savefig(head1+'/aloss_'+tail1+'_'+modelstr2+'.png')\n","        print(head1+'/aloss_'+tail1+'_'+modelstr2+'.png')\n","        np.save(head1+'/aloss_'+tail1+'_'+modelstr2+'.npy', history.history['loss'])\n","        np.save(head1+'/aloss_val_'+tail1+'_'+modelstr2+'.npy', history.history['val_loss'])\n","\n","\n","        plt.clf()\n","        plt.figure(figsize=(r1,c1))\n","        plt.rcParams.update({'font.size': f1})\n","        plt.plot(history.history['accuracy'],linewidth=linew1)\n","        plt.plot(history.history['val_accuracy'],linewidth=linew1)\n","        plt.title('Model Accuracy')\n","        plt.xlabel('epoch')\n","        plt.ylabel('accuracy')\n","        plt.legend(['train','test'])\n","    #    plt.show()\n","        plt.savefig(head1+'/acc_'+tail1+'_'+modelstr2+'.png')\n","    #    plt.clf()\n","        np.save(head1+'/acc_'+tail1+'_'+modelstr2+'.npy', history.history['accuracy'])\n","        np.save(head1+'/acc_val_'+tail1+'_'+modelstr2+'.npy', history.history['val_accuracy'])\n","\n","        y_pred = model.predict(x_test, batch_size = batch_size1)\n","        y_test1=np.argmax(y_test, axis=1)\n","    #    y_test1=y_test1.tolist()\n","        y_pred1 = np.argmax(y_pred, axis=1)\n","    #    y_pred1 = y_pred1.tolist()\n","        # predict probabilities for test set\n","        yhat_probs = y_pred\n","        # predict crisp classes for test set\n","        # yhat_classes = model.predict_classes(x_test, verbose=0)\n","        predict_x=model.predict(x_test)\n","        yhat_classes=np.argmax(predict_x,axis=1)\n","        # reduce to 1d array\n","        yhat_probs = yhat_probs[:, 0]\n","    #    yhat_classes = yhat_classes[:, 0]\n","        # accuracy: (tp + tn) / (p + n)\n","        accuracy = accuracy_score(y_test1, yhat_classes)\n","        print('Accuracy: %f' % accuracy)\n","        # precision tp / (tp + fp)\n","        precision = precision_score(y_test1, yhat_classes,average='weighted')\n","        print('Precision: %f' % precision)\n","        # recall: tp / (tp + fn)\n","        recall = recall_score(y_test1, yhat_classes,average='weighted')\n","        print('Recall: %f' % recall)\n","        # f1: 2 tp / (2 tp + fp + fn)\n","        f1 = f1_score(y_test1, yhat_classes,average='weighted')\n","        print('F1 score: %f' % f1)\n","        # kappa\n","        kappa = cohen_kappa_score(y_test1, yhat_classes)\n","        print('Cohens kappa: %f' % kappa)\n","        # ROC AUC\n","    #    auc = roc_auc_score(y_test1, yhat_probs)\n","    #    print('ROC AUC: %f' % auc)\n","        # confusion matrix\n","        conf_mat = confusion_matrix(y_test1, yhat_classes)\n","        df=pd.DataFrame(data=conf_mat[0:,0:], index=[i for i in range(conf_mat.shape[0])], columns=['f'+str(i) for i in range(conf_mat.shape[1])])\n","        df.to_excel(head1+'/aconf_mat_'+tail1+'_'+modelstr2+'.xlsx')\n","        print(conf_mat)\n","    #    np.savetxt(head1+'\\\\conf_mat_'+tail1+'_'+modelstr1+'.csv', conf_mat, '%s', delimiter=\",\")\n","\n","    #    labels=['ANGRY','HAPPY','NEUTRAL','SAD']\n","    #    print('_test_data_class: test function with y_test (actual values) and predictions (predict)')\n","    #    _test_data_class(y_test1,y_pred1)\n","        alist.loc[i]=[str1,tail1,'',score[1],score[0],precision,recall,f1,kappa,'']\n","        i=i+1\n","    #    np.savetxt(head1+'\\\\alist_'+'_'+modelstr1+'.csv', alist, '%s', delimiter=\"/t\")\n","        alist.to_excel(head1+'/alist_'+'_'+tail1+'_'+modelstr2+'.xlsx')\n","        del x_train\n","        del x_test\n"],"metadata":{"id":"OPcufiA273Ja","colab":{"base_uri":"https://localhost:8080/","height":245},"outputId":"c6685eb1-a7e2-4dd9-f0cf-737b685d0208","executionInfo":{"status":"error","timestamp":1709837479715,"user_tz":-180,"elapsed":2409,"user":{"displayName":"Salih Zeki Parlak","userId":"11379211521548434969"}}},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0.001\n","iemo full mfcc_emp3 _d __cons_512_256_AHNS.arff\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'x' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-daab1cfd35d6>\u001b[0m in \u001b[0;36m<cell line: 476>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;31m#    break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"]}]}]}